#!/usr/bin/env python

# pip install langchain-community langchain-ollama
# https://python.langchain.com/docs/integrations/chat/ollama/

from common import *
from langchain_ollama import ChatOllama
from llm_actor.langchain import Conversation
from llm_actor import Actor, Prolog

# model = "mistral:7b"
# model = "gemma3:4b"
# model = "llama3.1:8b"
# model = "deepseek-r1:8b"
model = "phi4:14b" # pretty good on mac
# model = "qwq:32b" # too slow on mac

space = "../llm-actor-spaces/spaces/001"

chat = ChatOllama(model=model, temperature=0, n_gpu_layers=-1)
prolog = Prolog("prologs/apprentice-system.1")
conv = Conversation(chat, model=model, prolog=prolog)
act = Actor(conv, space=space)

def ask(q):
    print(q)
    print(act.ask(q))

ask("How long (in lines) is the program in x1.c?")
ask("will the code compile? explain!")
act.print_summary()
